# KNN

## 一、 KNN 算法概述

KNN 算法的核心思想是基于“**物以类聚，人以群分**”的理念。它假设在特征空间中，一个样本的类别/值可以由它**最近的 $K$ 个邻居**的类别/值来决定。

- **监督学习（Supervised Learning）**：需要带有标签的训练数据集。
- **非参数（Non-parametric）**：它不对数据分布做任何假设，模型不会从训练数据中生成一个明确的判别函数，而是直接基于存储的实例进行学习。
- **惰性学习（Lazy Learning）**：在训练阶段，算法只是简单地将训练数据存储起来，而所有计算（分类或回归）都推迟到接收到新的待预测样本时才进行。

------

## 二、 KNN 算法的工作原理和流程

KNN 的工作流程非常直观，当接收到一个新的待分类/回归的样本时，主要步骤如下：

### 1. 确定 $K$ 值

首先，需要指定一个正整数 $K$ 值，它表示要考虑的**最近邻居的数量**。

### 2. 计算距离

计算新的待预测样本点（查询点）与训练数据集中**所有样本点**之间的距离。

### 3. 选取 $K$ 个最近邻

对计算出的距离进行排序，并选出距离最小的 $K$ 个训练样本点，这些点就是待预测样本的 $K$ 个最近邻居。

### 4. 做出预测

根据 $K$ 个最近邻居的属性，通过“多数表决”或“平均”的方式确定待预测样本的类别或值。

- **对于分类问题**：
  - 采取**多数表决**（Majority Vote）原则。
  - 将待预测样本的类别判定为 $K$ 个邻居中**出现频率最高的类别**。
  - *例如：若 $K=3$，最近的 3 个邻居中有 2 个属于“A”类，1 个属于“B”类，则待预测样本被判定为“A”类。*
- **对于回归问题**：
  - 采取**平均**（Averaging）原则。
  - 将 $K$ 个邻居的**目标值的平均值**作为待预测样本的预测值。
  - *也可以使用**加权平均**，距离越近的邻居赋予越大的权重。*

------

## 三、 KNN 算法的关键要素

### 1. 距离度量（Distance Metrics）

距离是衡量样本之间相似度的标准，不同的距离计算方法会影响最终的结果。

| **距离名称**                                                 | **适用参数 p** | **公式（以二维为例，点 $A=(x_1,x_2) 和 B=(y_1,y_2)$）**     |
| ------------------------------------------------------------ | -------------- | ----------------------------------------------------------- |
| **平方欧式距离（squared Euclidean）**                        |                | $D(X, Y) = \sum_{k=1}^{n} (x_k - y_k)^2$                    |
| **曼哈顿距离 (Manhattan Distance) ** i.e.*闵可夫斯基距离在 p=1 的特殊情况* | $p=1$          | $D_{\text{Manhattan}}(X, Y) = \sum_{i=1}^{n}|x_i - y_i| $   |
| **闵可夫斯基距离 (Minkowski Distance)**                      | $p \geq 1$     | $D(X, Y) = (\sum_{i=1}^{n}{|x_i - y_i|^p})^{\frac {1}{p}} $ |

### 2. $K$ 值的选择（Choosing $K$）

$K$ 值的选择是 KNN 算法中最重要的超参数之一，它直接影响模型的性能：

- **$K$ 值过小**：
  - 模型变得**复杂**，容易受到**局部噪声**和**异常值**的影响，导致**过拟合**（高方差，低偏差）。
- **$K$ 值过大**：
  - 模型的覆盖范围变大，可能将较远的、不相似的样本也纳入决策，导致模型的**泛化能力下降**（高偏差，低方差），甚至使决策边界趋于平滑，丧失局部细节。

通常，可以通过**交叉验证（Cross-Validation）**的方法，选取一个最优的 $K$ 值。经验上， $K$ 值通常取一个较小的整数（如 $K$ 不超过 $\sqrt{n}$，其中 $n$ 是样本总数）。

------

## 四、 KNN 算法的优缺点

### 👍 优点

- **简单易懂**：原理简单直观，易于实现。
- **无需训练**：训练阶段开销为零，只需存储数据，是“惰性学习”的典型代表。
- **对数据分布无假设**：是一种非参数方法，适用于不同类型的数据集。
- **精度高**：在大数据集和平衡数据集上，具有较高的准确率。

### 👎 缺点

- **计算量大**：在预测阶段，需要计算待预测点与所有训练样本的距离，对于**大规模数据集**和**高维数据**，计算开销和时间成本很高。
- **对内存要求高**：需要存储所有的训练数据。
- **对特征尺度敏感**：由于是基于距离的算法，特征的量纲（单位）差异会影响距离计算，因此通常需要进行**特征归一化或标准化**。
- **不适用于样本不均衡**：如果样本类别不均衡，多数类别的样本会占据主导地位，可能导致分类器偏向多数类。